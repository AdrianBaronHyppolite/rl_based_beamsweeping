{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 10:14:33.501185: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2023-05-25 10:14:33.501218: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import copy \n",
    "from newenv import RLIAEnv\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#states = random states generated for training, \n",
    "#total_states = possible states count\n",
    "def ohe_generator(states,total_states):\n",
    "    ohe = np.zeros((len(states),total_states))\n",
    "    for index, array in enumerate(ohe):\n",
    "        ohe[index][states[index]] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandits:\n",
    "    def __init__(self,states,actions):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "    \n",
    "    def reward(self,state,action):\n",
    "        if (state*action)%2==1:\n",
    "            return 0.5 + 0.05*((state+action)%10)+np.random.rand()*0.1\n",
    "        else:\n",
    "            return 0.9 - 0.1*((state+action)%10)+np.random.rand()*0.1\n",
    "    \n",
    "    def network(self):\n",
    "        input_ = Input(shape=(self.states))\n",
    "        dense1 = Dense(128,activation='relu')(input_)\n",
    "        dropout1 = Dropout(0.1)(dense1)\n",
    "        dense2 = Dense(64,activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.1)(dense2)\n",
    "        dense3 = Dense(self.actions,activation='sigmoid')(dropout2)\n",
    "        model = Model(input_,dense3)\n",
    "        \n",
    "        rms = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        model.compile(loss=\"mean_absolute_error\", optimizer=rms,metrics=\"mean_absolute_error\")\n",
    "        return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLIAEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "states = 100\n",
    "actions = 3\n",
    "\n",
    "def training():\n",
    "    cb = contextual_bandits(env.state, actions)\n",
    "    model = cb.network()\n",
    "    sample_states = np.random.choice(range(env.state),size=batch_size*100)\n",
    "    state_ohe = ohe_generator(sample_states, states)\n",
    "    actual_reward = [[cb.reward(x,y) for y in range(cb.actions)] for x in sample_states]\n",
    "    actual_reward_matrix = np.zeros((len(state_ohe),cb.actions))\n",
    "    for index,x in enumerate(actual_reward):\n",
    "                    actual_reward_matrix[index]=np.array(x)\n",
    "    model.fit(state_ohe,actual_reward_matrix,batch_size=batch_size,epochs=20) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m training()\n",
      "\u001b[1;32m/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb Cell 6\u001b[0m in \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cb \u001b[39m=\u001b[39m contextual_bandits(env\u001b[39m.\u001b[39mstate, actions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m cb\u001b[39m.\u001b[39mnetwork()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m sample_states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mrange\u001b[39;49m(env\u001b[39m.\u001b[39;49mstate),size\u001b[39m=\u001b[39;49mbatch_size\u001b[39m*\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m state_ohe \u001b[39m=\u001b[39m ohe_generator(sample_states, states)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adrian/Documents/rl_based_beamsweeping/src/newmabtest.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m actual_reward \u001b[39m=\u001b[39m [[cb\u001b[39m.\u001b[39mreward(x,y) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cb\u001b[39m.\u001b[39mactions)] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sample_states]\n",
      "File \u001b[0;32mmtrand.pyx:915\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "a = training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
